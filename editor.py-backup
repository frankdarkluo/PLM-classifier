from transformers import RobertaTokenizer, RobertaForMaskedLM,pipeline
import torch
import numpy as np
import pytorch_lightning as pl
import RAKE
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
import nltk
#nltk.download('averaged_perceptron_tagger')
# rev_word_pairs = {"it 's": "it is", "do n't": "do not", "does n't": "does not", "did n't": "did not","you 'd": "you would",
#                       "you 're": "you are", "you 'll": "you will", "we \'ll": "we'll", "i 'm": "i am","I 'd": "I would",
#                       "they 're": "they are", "that 's": "that is","what 's": "what is", "could n't": "could not", "i 've": "i have",
#                       "we 've": "we have", "ca n't": "can not", "i 'd": "i would", "are n't": "are not", "is n't": "is not",
#                       "was n't": "was not", "would n't": "would not","were n't": "were not", "wo n't": "will not", "she 's": "she is",
#                       "there 's": "there is", "there 're": "there are", "i 'll": "i will","he 'd":"he would", "he 's":"he is", }

class RobertaEditor(pl.LightningModule):
    def __init__(self, opt):
        super(RobertaEditor, self).__init__()
        self.save_hyperparameters(opt)
        self.opt=opt
        self.model_dir = opt.model_name_or_path
        self.topk=opt.topk
        self.model = RobertaForMaskedLM.from_pretrained(self.model_dir, return_dict=True).to(device)
        self.tokenizer = RobertaTokenizer.from_pretrained(self.model_dir)
        #self.add_some_special_tokens()
        self.model.resize_token_embeddings(len(self.tokenizer))
        self.ops_map = [self.insert, self.replace, self.delete]
        self.unmasker = pipeline("fill-mask", model=self.model_dir)
        self.max_len = opt.max_input_length
        self.Rake = RAKE.Rake(RAKE.SmartStopList())

        print("Editor built")
    
    def edit(self, inputs, ops, positions,max_len=None):

        masked_inputs = np.array([self.ops_map[op](inp, position) for inp, op, position, in zip(inputs, ops, positions)])
        if ops<2: # replacement or insertion, have a mask
            _inputs = masked_inputs
            _outputs = self.generate(_inputs.tolist(),max_len)
            masked_inputs = _outputs # it is a list of top_k sents -->[sent1, sent2,..., sent_k]

        return masked_inputs

    def generate(self, input_texts,max_len):

        sent_list=[]
        mask_words = [output['token_str'].strip() for output in self.unmasker(input_texts,top_k=self.topk)]
        for input_text in input_texts:
            for mask_word in mask_words:
                cand_sent = input_text.replace("<mask>", mask_word)
                cand_sent = ' '.join(cand_sent.split(' ')[:max_len])
                sent_list.append(cand_sent)

        return sent_list

    def insert(self, input_texts, mask_idx):
        input_texts_with_mask_list = input_texts.split()[:mask_idx] + ["<mask>"] + input_texts.split()[mask_idx:]
        return " ".join(input_texts_with_mask_list)

    def replace(self, input_texts, mask_idx):
        input_texts_with_mask_list = input_texts.split()[:mask_idx] + ["<mask>"] + input_texts.split()[mask_idx + 1:]
        return " ".join(input_texts_with_mask_list)

    def delete(self, input_texts, mask_idx):
        input_texts_with_mask_list = input_texts.split()[:mask_idx] + input_texts.split()[mask_idx + 1:]
        return " ".join(input_texts_with_mask_list)

    def rbt_state_vec(self,rbt_token_lines,pos):
        rbt_pos_list = []
        pos_index = 0
        for idx, token in enumerate(rbt_token_lines):
            if idx == 0:
                rbt_pos_list.append(pos[pos_index])
                pos_index += 1
            else:  # idx>1
                if 'Ġ' in rbt_token_lines[idx]:
                    rbt_pos = pos[pos_index]
                    rbt_pos_list.append(rbt_pos)
                    pos_index += 1
                elif 'Ġ' not in rbt_token_lines[idx]:
                    #follow the previous token pos tag
                    rbt_pos = pos[pos_index-1]
                    rbt_pos_list.append(rbt_pos)

        return rbt_pos_list

    def state_vec(self, inputs):
        sta_vec_list = []
        pos_list = []
        for line in inputs:
            line = line.strip().lower()
            # for k, v in rev_word_pairs.items():
            #     line = line.replace(k, v)
            line = ' '.join(line.split()[:self.max_len])
            rbt_line = [token for token in self.tokenizer.tokenize(line)]

            sta_vec = list(np.zeros([self.max_len]))
            keyword = self.Rake.run(line.strip())
            # pos_list = self.tagger.tag_sentence(line.strip()).split()
            pos_tags = nltk.tag.pos_tag(line.split())
            pos = [x[1] for x in pos_tags]
            pos_list.append(pos)
            rbt_pos_list = self.rbt_state_vec(rbt_line, pos)

            # if keyword != []:
            #     keyword = list(list(zip(*keyword))[0])
            #     keyword_new = []
            #     linewords = line.split()
            #     for i in range(len(linewords)):
            #         for item in keyword:
            #             length11 = len(item.split())
            #             if ' '.join(linewords[i:i + length11]) == item:
            #                 keyword_new.extend([i + k for k in range(length11)])
            #     for i in range(len(keyword_new)):
            #         ind = keyword_new[i]
            #         if ind <= self.max_len - 2:
            #             sta_vec[ind] = 1
            # if self.opt.keyword_pos == True:
            #     sta_vec_list.append(self.keyword_pos2sta_vec(self.opt, sta_vec, pos))
            # else:
            #     sta_vec_list.append(list(np.zeros([self.max_len - 1])))
            rbt_state_vec_list=list(np.zeros([self.max_len]))
            for idx,pos in enumerate(rbt_pos_list[:self.max_len]):
                if pos in ['JJS', 'JJR', 'JJ', 'RBR', 'RBS', 'RB', 'VBZ', 'VBP', 'VBN', 'VBG', 'VBD', 'VB']:
                    rbt_state_vec_list[idx]=1

        return rbt_state_vec_list, pos_list

    def mean_pooling(self,model_output, attention_mask):
        token_embeddings = model_output[0]  # First element of model_output contains all token embeddings
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

    def get_contextual_word_embeddings(self, input_texts):
        inputs = {k:v.to('cuda') for k,v in self.tokenizer(input_texts, padding=True, return_tensors="pt").items()}
        outputs = self.model(**inputs, output_hidden_states=True)
        sentence_embeddings = self.mean_pooling(outputs, inputs['attention_mask'])
        if outputs.hidden_states[-1].shape[1]>=22:
            hidden_states=outputs.hidden_states[-1][:,1:21, :]
        else:
            hidden_states = outputs.hidden_states[-1][:, 1:-1, :]
        return hidden_states,sentence_embeddings

    def keyword_pos2sta_vec(self,option, keyword, pos):
        key_ind = []
        # pos=pos[:option.num_steps-1]
        pos = pos[:self.max_len - 1]
        for i in range(len(pos)):
            # if pos[i] in ['NN', 'NNS','NNP','NNPS']:
            #     key_ind.append(i)
            if pos[i] in ['NN', 'NNS','NNP','NNPS'] and keyword[i] == 1:
                key_ind.append(i)
            # elif pos[i] in ['VBZ'] and keyword[i] == 1:
            #     key_ind.append(i)
            # elif pos[i] in ['VBZ', 'VBP', 'VBN', 'VBG', 'VBD', 'VB'] and keyword[i] == 1:
            #     key_ind.append(i)

            if pos[i] in ['JJS', 'JJR', 'JJ', 'RBR', 'RBS', 'RB', 'VBZ', 'VBP', 'VBN', 'VBG', 'VBD', 'VB']:
                key_ind.append(i)

        key_ind = key_ind[:max(int(option.max_key_rate * len(pos)), option.max_key)]
        sta_vec = []
        for i in range(len(keyword)):
            if i in key_ind:
                sta_vec.append(1)
            else:
                sta_vec.append(0)
        # liuxg
        if np.sum(sta_vec) == 0:
            sta_vec[0] = 1
        return sta_vec